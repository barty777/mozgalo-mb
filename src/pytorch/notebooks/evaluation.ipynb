{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from data import crop_upper_part\n",
    "from model import SqueezeModelSoftmax\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pickle\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a PyTorch state dict\n",
    "MODEL_PATH = '../models/Standard-No-Test/26-no-test_epoch_10-valLoss_0.02410-valF1_0.99846'  \n",
    "THRESHOLDS = \"../models/thresholds_2std.pkl\"\n",
    "NUM_CLASSES = 26\n",
    "INPUT_SHAPE = (3, 370, 400) # C x H x W\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# path to a non-annotated dataset where all images are in same folder with name: <integer_id>.jpg\n",
    "DATASET_PATH = '../../data/robust_ml_challenge_testset'\n",
    "\n",
    "# Annotated test dataset path\n",
    "ANNOTATED_TEST_DATASET = '../../data//test_class_annot_better/test_class'\n",
    "\n",
    "# path to empty folder (it is not mandatory that it exists) - where annotated dataset will be stored\n",
    "OUTPUT_ANNOTATED_PATH = '../../data/annotated_ds'\n",
    "\n",
    "OUTPUT_CSV_PATH = '../output_no_test_focal.csv'\n",
    "\n",
    "NUM_THREADS = 4 # number of threads to use - should be same as number of virtual CPU cores\n",
    "USE_GPU = True # use CUDA related stuff\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    " 'Albertsons': 0,\n",
    " 'BJs': 1,\n",
    " 'CVSPharmacy': 2,\n",
    " 'Costco': 3,\n",
    " 'FredMeyer': 4,\n",
    " 'Frys': 5,\n",
    " 'HEB': 6,\n",
    " 'HarrisTeeter': 7,\n",
    " 'HyVee': 8,\n",
    " 'JewelOsco': 9,\n",
    " 'KingSoopers': 10,\n",
    " 'Kroger': 11,\n",
    " 'Meijer': 12,\n",
    " 'Other': 13,\n",
    " 'Publix': 14,\n",
    " 'Safeway': 15,\n",
    " 'SamsClub': 16,\n",
    " 'ShopRite': 17,\n",
    " 'Smiths': 18,\n",
    " 'StopShop': 19,\n",
    " 'Target': 20,\n",
    " 'Walgreens': 21,\n",
    " 'Walmart': 22,\n",
    " 'Wegmans': 23,\n",
    " 'WholeFoodsMarket': 24,\n",
    " 'WinCoFoods': 25}\n",
    "\n",
    "class_dict = {v: k for k, v in class_labels.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_transformations(input_shape, crop_perc = 0.5):\n",
    "    \"\"\"Preprocess object for transforming image to model input\n",
    "    Args:\n",
    "        input_shape: model input shape (channels x height x width)\n",
    "        crop_perc: percent of how much image would be cropped from\n",
    "\n",
    "    Returns:\n",
    "        Composite of transforms objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_channels, height, width = input_shape\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(lambda x: crop_upper_part(np.array(x), crop_perc)),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(num_channels),\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "def list_input_images(images_folder):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images_folder: Folder with input images with name template: <int_id>.jpg\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (image_file_name, image_int_id)\n",
    "    \"\"\"\n",
    "    files = os.listdir(images_folder)\n",
    "    images = []\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        name_components = file.split(\".\")\n",
    "        extension = name_components[1].lower()\n",
    "        \n",
    "        if extension == 'jpg' or extension == 'jpeg':\n",
    "            image_id = int(name_components[0])\n",
    "            images.append((file, image_id))\n",
    "            \n",
    "    return sorted(images, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "def open_image(image_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_path: Path to an image.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image in RGB format.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image = Image.open(f).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def predicted_store(prediction):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction: Model probability output.\n",
    "        \n",
    "    Returns:\n",
    "        Most probable store (argmax)\n",
    "    \"\"\"\n",
    "    class_indice = np.argmax(prediction)\n",
    "    return class_dict[class_indice]\n",
    "\n",
    "\n",
    "def print_predicted_store_stats(predicted_stores):\n",
    "    unique, counts = np.unique(predicted_stores, return_counts=True)\n",
    "    stats_dict = dict(zip(unique, counts))\n",
    "    for store, count in stats_dict.items():\n",
    "        print(\"{:<16} => {}\".format(store, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Store recipts test dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images = list_input_images(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.images[idx][0])\n",
    "        image = open_image(image_path)\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SqueezeModelSoftmax(num_classes=NUM_CLASSES)\n",
    "model_state_dict = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()\n",
    "torch.set_num_threads(NUM_THREADS)\n",
    "\n",
    "if USE_GPU:\n",
    "    model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 157/157\r"
     ]
    }
   ],
   "source": [
    "def var(tensor):\n",
    "    if USE_GPU:\n",
    "        tensor = tensor.cuda(0)\n",
    "    return tensor\n",
    "\n",
    "predictions = []\n",
    "preprocess_transformations = data_preprocess_transformations(INPUT_SHAPE)\n",
    "test_set = TestDataset(DATASET_PATH, preprocess_transformations)\n",
    "loader = DataLoader(test_set,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=NUM_THREADS)\n",
    "\n",
    "num_batches = len(loader)\n",
    "for batch_index, test_batch in enumerate(loader):\n",
    "    \n",
    "    batch_input_tensors = var(test_batch)\n",
    "\n",
    "    batch_predictions = model(batch_input_tensors).cpu().data.numpy()\n",
    "    predictions.extend(batch_predictions)\n",
    "    \n",
    "    print('Batch {}/{}'.format(batch_index + 1, num_batches), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on the annotated test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare loader for the annotated test set\n",
    "test_transform = data_preprocess_transformations(INPUT_SHAPE)\n",
    "test_dataset = ImageFolder(root=ANNOTATED_TEST_DATASET,                                                  \n",
    "                                      transform=test_transform)                                                                           \n",
    "    \n",
    "test_loader = DataLoader(test_dataset,                              \n",
    "                        batch_size=BATCH_SIZE,                                                                  \n",
    "                        shuffle=False,                                                                               \n",
    "                        num_workers=NUM_THREADS)                                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 157/157\r"
     ]
    }
   ],
   "source": [
    "def var(tensor):\n",
    "    if USE_GPU:\n",
    "        tensor = tensor.cuda(0)\n",
    "    return tensor\n",
    "\n",
    "# Run prediction\n",
    "predictions, gts = [], []\n",
    "num_batches = len(test_loader)\n",
    "\n",
    "for batch_index, (test_batch, ground_truth) in enumerate(test_loader):\n",
    "    batch_input_tensors = var(test_batch)\n",
    "\n",
    "    batch_predictions = model(batch_input_tensors).cpu().data.numpy()\n",
    "    predictions.extend(batch_predictions)\n",
    "    gts.extend(ground_truth.cpu().data.numpy())\n",
    "    \n",
    "    print('Batch {}/{}'.format(batch_index + 1, num_batches), end=\"\\r\", flush=True)\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Heuristics \n",
    "\n",
    "# Load thresholds calculated on the validation set\n",
    "with open(THRESHOLDS, \"rb\") as f:\n",
    "    thr_dict = pickle.load(f)\n",
    "\n",
    "def threshold_heuristic(predictions, thr_dict):\n",
    "    \"\"\"\n",
    "    Per-class threshold heuristic\n",
    "    \"\"\"\n",
    "    new_preds = []\n",
    "    \n",
    "    for act in predictions:\n",
    "        pred_label = np.argmax(act)\n",
    "        if class_dict[pred_label] != \"Other\":\n",
    "            if np.max(act) < thr_dict[class_dict[pred_label]]:\n",
    "                # Replace as if the Other is predicted because it doesn't satsify the threshold\n",
    "                other_vector = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                other_vector[class_labels[\"Other\"]] = 1\n",
    "                \n",
    "                new_preds.append(other_vector)\n",
    "            else:\n",
    "                new_preds.append(act)\n",
    "        else:\n",
    "            new_preds.append(act)\n",
    "            \n",
    "    return new_preds\n",
    "\n",
    "def threshold_heuristic_fixed(predictions, thr):\n",
    "    \"\"\"\n",
    "    Per-class threshold heuristic\n",
    "    \"\"\"\n",
    "    new_preds = []\n",
    "    \n",
    "    for act in predictions:\n",
    "        pred_label = np.argmax(act)\n",
    "        if class_dict[pred_label] != \"Other\":\n",
    "            if np.max(act) < thr:\n",
    "                # Replace as if the Other is predicted because it doesn't satsify the threshold\n",
    "                other_vector = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "                other_vector[class_labels[\"Other\"]] = 1\n",
    "                \n",
    "                new_preds.append(other_vector)\n",
    "            else:\n",
    "                new_preds.append(act)\n",
    "        else:\n",
    "            new_preds.append(act)\n",
    "            \n",
    "    return new_preds\n",
    "\n",
    "def identity_heuristic(predictions):\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply heuristics and Lets make predictions\n",
    "\n",
    "predictions_new = threshold_heuristic(predictions, thr_dict=thr_dict)\n",
    "predicted_stores = [predicted_store(prediction) for prediction in predictions_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on the annotated test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.64      0.77       200\n",
      "          1       1.00      0.97      0.98       200\n",
      "          2       1.00      0.98      0.99       200\n",
      "          3       1.00      0.91      0.95       200\n",
      "          4       0.99      0.89      0.94       200\n",
      "          5       1.00      0.83      0.91       201\n",
      "          6       1.00      0.89      0.94       200\n",
      "          7       1.00      0.48      0.65       203\n",
      "          8       1.00      0.83      0.91       199\n",
      "          9       0.76      0.95      0.85       204\n",
      "         10       1.00      0.71      0.83       200\n",
      "         11       1.00      0.64      0.78       203\n",
      "         12       1.00      0.72      0.84       200\n",
      "         13       0.84      0.97      0.90      4990\n",
      "         14       1.00      0.63      0.77       200\n",
      "         15       0.99      0.66      0.79       199\n",
      "         16       1.00      0.93      0.96       200\n",
      "         17       1.00      0.93      0.96       201\n",
      "         18       1.00      0.87      0.93       200\n",
      "         19       0.65      0.84      0.73       199\n",
      "         20       1.00      0.84      0.92       200\n",
      "         21       1.00      0.69      0.82       200\n",
      "         22       0.99      0.86      0.93       200\n",
      "         23       1.00      0.86      0.93       200\n",
      "         24       1.00      0.88      0.93       200\n",
      "         25       1.00      0.86      0.93       201\n",
      "\n",
      "avg / total       0.91      0.89      0.89     10000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.69      0.80       200\n",
      "          1       1.00      0.99      0.99       200\n",
      "          2       1.00      0.98      0.99       200\n",
      "          3       1.00      0.96      0.98       200\n",
      "          4       0.99      0.91      0.95       200\n",
      "          5       1.00      0.93      0.96       201\n",
      "          6       1.00      0.97      0.99       200\n",
      "          7       1.00      0.67      0.80       203\n",
      "          8       0.98      0.94      0.96       199\n",
      "          9       0.89      0.92      0.91       204\n",
      "         10       1.00      0.80      0.89       200\n",
      "         11       0.94      0.79      0.86       203\n",
      "         12       0.91      0.90      0.90       200\n",
      "         13       0.89      0.97      0.93      4990\n",
      "         14       0.99      0.69      0.81       200\n",
      "         15       0.99      0.84      0.91       199\n",
      "         16       1.00      0.95      0.98       200\n",
      "         17       0.99      0.97      0.98       201\n",
      "         18       1.00      0.93      0.96       200\n",
      "         19       0.65      0.88      0.75       199\n",
      "         20       0.99      0.92      0.95       200\n",
      "         21       1.00      0.77      0.87       200\n",
      "         22       0.96      0.91      0.93       200\n",
      "         23       0.99      0.96      0.98       200\n",
      "         24       1.00      0.94      0.97       200\n",
      "         25       1.00      0.96      0.98       201\n",
      "\n",
      "avg / total       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply heuristics\n",
    "predictions_new = threshold_heuristic(predictions, thr_dict=thr_dict)\n",
    "predictions_new_fix = threshold_heuristic_fixed(predictions, thr=0.7)\n",
    "\n",
    "print(classification_report(gts, np.argmax(predictions_new, axis=1)))\n",
    "print(classification_report(gts, np.argmax(predictions_new_fix, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and annotate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted stores to CSV\n",
    "\n",
    "with open(OUTPUT_CSV_PATH, \"w\") as f:\n",
    "    for i, store in enumerate(predicted_stores):\n",
    "        f.write(store)\n",
    "        if i < len(predicted_stores) - 1:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WholeFoodsMarket => 183\n",
      "Publix           => 154\n",
      "CVSPharmacy      => 197\n",
      "Meijer           => 162\n",
      "Other            => 5259\n",
      "Walmart          => 183\n",
      "HEB              => 191\n",
      "WinCoFoods       => 188\n",
      "Wegmans          => 183\n",
      "Kroger           => 143\n",
      "BJs              => 198\n",
      "JewelOsco        => 447\n",
      "Safeway          => 151\n",
      "Costco           => 189\n",
      "ShopRite         => 195\n",
      "Target           => 185\n",
      "FredMeyer        => 192\n",
      "Smiths           => 180\n",
      "SamsClub         => 191\n",
      "HyVee            => 173\n",
      "KingSoopers      => 177\n",
      "Frys             => 186\n",
      "Albertsons       => 132\n",
      "HarrisTeeter     => 120\n",
      "Walgreens        => 163\n",
      "StopShop         => 278\n",
      "\n",
      "WholeFoodsMarket => 192\n",
      "Publix           => 150\n",
      "CVSPharmacy      => 198\n",
      "Meijer           => 211\n",
      "Other            => 5204\n",
      "Walmart          => 195\n",
      "HEB              => 197\n",
      "WinCoFoods       => 193\n",
      "Wegmans          => 196\n",
      "Kroger           => 185\n",
      "BJs              => 199\n",
      "JewelOsco        => 228\n",
      "Safeway          => 178\n",
      "Costco           => 196\n",
      "ShopRite         => 196\n",
      "Target           => 188\n",
      "FredMeyer        => 192\n",
      "Smiths           => 191\n",
      "SamsClub         => 192\n",
      "HyVee            => 200\n",
      "KingSoopers      => 172\n",
      "Frys             => 191\n",
      "Albertsons       => 171\n",
      "HarrisTeeter     => 143\n",
      "Walgreens        => 165\n",
      "StopShop         => 277\n"
     ]
    }
   ],
   "source": [
    "# Print current statistics\n",
    "\n",
    "print_predicted_store_stats(predicted_stores)\n",
    "print()\n",
    "print_predicted_store_stats(predicted_stores_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate - copy images to new folder - separated by class\n",
    "\n",
    "if not os.path.exists(OUTPUT_ANNOTATED_PATH):\n",
    "    os.makedirs(OUTPUT_ANNOTATED_PATH)\n",
    "    \n",
    "num_stores = len(predicted_stores)\n",
    "    \n",
    "for index, store in enumerate(predicted_stores):\n",
    "    target_folder_path = os.path.join(OUTPUT_ANNOTATED_PATH, store)\n",
    "    if not os.path.exists(target_folder_path):\n",
    "        os.makedirs(target_folder_path)\n",
    "    \n",
    "    filename = str(index) + \".jpg\"\n",
    "    src_file_path = os.path.join(DATASET_PATH, filename)\n",
    "\n",
    "    shutil.copy2(src_file_path, target_folder_path)\n",
    "    print('Image {}/{}'.format(index + 1, num_stores), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
