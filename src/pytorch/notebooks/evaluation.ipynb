{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "from data import crop_upper_part\n",
    "from model import SqueezeModelSoftmax\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to a PyTorch state dict\n",
    "MODEL_PATH = '../models/26-no-test-focal-loss_epoch_16-valLoss_4.20528-valF1_0.99330' \n",
    "NUM_CLASSES = 26\n",
    "INPUT_SHAPE = (3, 370, 400) # C x H x W\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# path to a non-annotated dataset where all images are in same folder with name: <integer_id>.jpg\n",
    "DATASET_PATH = '../../data/robust_ml_challenge_testset'\n",
    "\n",
    "# path to empty folder (it is not mandatory that it exists) - where annotated dataset will be stored\n",
    "OUTPUT_ANNOTATED_PATH = '../../data/annotated_ds'\n",
    "\n",
    "OUTPUT_CSV_PATH = '../output_no_test_focal.csv'\n",
    "\n",
    "NUM_THREADS = 4 # number of threads to use - should be same as number of virtual CPU cores\n",
    "USE_GPU = True # use CUDA related stuff\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    " 'Albertsons': 0,\n",
    " 'BJs': 1,\n",
    " 'CVSPharmacy': 2,\n",
    " 'Costco': 3,\n",
    " 'FredMeyer': 4,\n",
    " 'Frys': 5,\n",
    " 'HEB': 6,\n",
    " 'HarrisTeeter': 7,\n",
    " 'HyVee': 8,\n",
    " 'JewelOsco': 9,\n",
    " 'KingSoopers': 10,\n",
    " 'Kroger': 11,\n",
    " 'Meijer': 12,\n",
    " 'Other': 13,\n",
    " 'Publix': 14,\n",
    " 'Safeway': 15,\n",
    " 'SamsClub': 16,\n",
    " 'ShopRite': 17,\n",
    " 'Smiths': 18,\n",
    " 'StopShop': 19,\n",
    " 'Target': 20,\n",
    " 'Walgreens': 21,\n",
    " 'Walmart': 22,\n",
    " 'Wegmans': 23,\n",
    " 'WholeFoodsMarket': 24,\n",
    " 'WinCoFoods': 25}\n",
    "\n",
    "class_dict = {v: k for k, v in class_labels.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess_transformations(input_shape, crop_perc = 0.5):\n",
    "    \"\"\"Preprocess object for transforming image to model input\n",
    "    Args:\n",
    "        input_shape: model input shape (channels x height x width)\n",
    "        crop_perc: percent of how much image would be cropped from\n",
    "\n",
    "    Returns:\n",
    "        Composite of transforms objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_channels, height, width = input_shape\n",
    "    \n",
    "    return transforms.Compose([\n",
    "        transforms.Lambda(lambda x: crop_upper_part(np.array(x), crop_perc)),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(num_channels),\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "def list_input_images(images_folder):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images_folder: Folder with input images with name template: <int_id>.jpg\n",
    "\n",
    "    Returns:\n",
    "        List of tuples: (image_file_name, image_int_id)\n",
    "    \"\"\"\n",
    "    files = os.listdir(images_folder)\n",
    "    images = []\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        name_components = file.split(\".\")\n",
    "        extension = name_components[1].lower()\n",
    "        \n",
    "        if extension == 'jpg' or extension == 'jpeg':\n",
    "            image_id = int(name_components[0])\n",
    "            images.append((file, image_id))\n",
    "            \n",
    "    return sorted(images, key=lambda x: x[1])\n",
    "\n",
    "\n",
    "def open_image(image_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image_path: Path to an image.\n",
    "\n",
    "    Returns:\n",
    "        PIL Image in RGB format.\n",
    "    \"\"\"\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image = Image.open(f).convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def predicted_store(prediction):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction: Model probability output.\n",
    "        \n",
    "    Returns:\n",
    "        Most probable store (argmax)\n",
    "    \"\"\"\n",
    "    class_indice = np.argmax(prediction)\n",
    "    return class_dict[class_indice]\n",
    "\n",
    "\n",
    "def print_predicted_store_stats(predicted_stores):\n",
    "    unique, counts = np.unique(predicted_stores, return_counts=True)\n",
    "    stats_dict = dict(zip(unique, counts))\n",
    "    for store, count in stats_dict.items():\n",
    "        print(\"{:<16} => {}\".format(store, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Store recipts test dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images = list_input_images(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.root_dir, self.images[idx][0])\n",
    "        image = open_image(image_path)\n",
    "        image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and make predictions (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SqueezeModelSoftmax(num_classes=NUM_CLASSES)\n",
    "model_state_dict = torch.load(MODEL_PATH, map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.eval()\n",
    "torch.set_num_threads(NUM_THREADS)\n",
    "\n",
    "if USE_GPU:\n",
    "    model.cuda(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 313/313\r"
     ]
    }
   ],
   "source": [
    "def var(tensor):\n",
    "    if USE_GPU:\n",
    "        tensor = tensor.cuda(0)\n",
    "    return tensor\n",
    "\n",
    "predictions = []\n",
    "preprocess_transformations = data_preprocess_transformations(INPUT_SHAPE)\n",
    "test_set = TestDataset(DATASET_PATH, preprocess_transformations)\n",
    "loader = DataLoader(test_set,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=NUM_THREADS)\n",
    "\n",
    "num_batches = len(loader)\n",
    "for batch_index, test_batch in enumerate(loader):\n",
    "    \n",
    "    batch_input_tensors = var(test_batch)\n",
    "\n",
    "    batch_predictions = model(batch_input_tensors).cpu().data.numpy()\n",
    "    predictions.extend(batch_predictions)\n",
    "    \n",
    "    print('Batch {}/{}'.format(batch_index + 1, num_batches), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Heuristics \n",
    "\n",
    "def some_heuristic_take_all_outputs(predictions):\n",
    "    return predictions\n",
    "\n",
    "predictions = some_heuristic_take_all_outputs(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make predictions\n",
    "\n",
    "predicted_stores = [predicted_store(prediction) for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and annotate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predicted stores to CSV\n",
    "\n",
    "with open(OUTPUT_CSV_PATH, \"w\") as f:\n",
    "    for i, store in enumerate(predicted_stores):\n",
    "        f.write(store)\n",
    "        if i < len(predicted_stores) - 1:\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albertsons       => 329\n",
      "BJs              => 201\n",
      "CVSPharmacy      => 207\n",
      "Costco           => 199\n",
      "FredMeyer        => 500\n",
      "Frys             => 188\n",
      "HEB              => 282\n",
      "HarrisTeeter     => 118\n",
      "HyVee            => 234\n",
      "JewelOsco        => 666\n",
      "KingSoopers      => 499\n",
      "Kroger           => 1151\n",
      "Meijer           => 309\n",
      "Other            => 698\n",
      "Publix           => 251\n",
      "Safeway          => 202\n",
      "SamsClub         => 205\n",
      "ShopRite         => 205\n",
      "Smiths           => 616\n",
      "StopShop         => 274\n",
      "Target           => 234\n",
      "Walgreens        => 695\n",
      "Walmart          => 911\n",
      "Wegmans          => 198\n",
      "WholeFoodsMarket => 402\n",
      "WinCoFoods       => 226\n"
     ]
    }
   ],
   "source": [
    "# Print current statistics\n",
    "\n",
    "print_predicted_store_stats(predicted_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 10000/10000\r"
     ]
    }
   ],
   "source": [
    "# Annotate - copy images to new folder - separated by class\n",
    "\n",
    "if not os.path.exists(OUTPUT_ANNOTATED_PATH):\n",
    "    os.makedirs(OUTPUT_ANNOTATED_PATH)\n",
    "    \n",
    "num_stores = len(predicted_stores)\n",
    "    \n",
    "for index, store in enumerate(predicted_stores):\n",
    "    target_folder_path = os.path.join(OUTPUT_ANNOTATED_PATH, store)\n",
    "    if not os.path.exists(target_folder_path):\n",
    "        os.makedirs(target_folder_path)\n",
    "    \n",
    "    filename = str(index) + \".jpg\"\n",
    "    src_file_path = os.path.join(DATASET_PATH, filename)\n",
    "\n",
    "    shutil.copy2(src_file_path, target_folder_path)\n",
    "    print('Image {}/{}'.format(index + 1, num_stores), end=\"\\r\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
